> **参考**：
> * [[读源码] Unsupervised Monocular Depth Estimation with Left-Right Consistency](https://zhuanlan.zhihu.com/p/29664269)
> * [Python的namedtuple使用详解](https://blog.csdn.net/kongxx/article/details/51553362)
> * [tensorflow的数据输入](https://blog.csdn.net/zzk1995/article/details/54292859)
> * [TensorFlow学习笔记（11）：数据操作指南](https://segmentfault.com/a/1190000008793389)
> * [一个很有用的函数 tf.py_func](https://zhuanlan.zhihu.com/p/32970370)
> * [tf.train.shuffle_batch函数解析](https://blog.csdn.net/u013555719/article/details/77679964)
> * [TensorFlow入门（七） 充分理解 name / variable_scope](https://blog.csdn.net/Jerr__y/article/details/70809528)
> * [Unsupervised Monocular Depth Estimation
with Left-Right Consistency](http://visual.cs.ucl.ac.uk/pubs/monoDepth/)
>* [论文阅读笔记之Dispnet](https://blog.csdn.net/kongfy4307/article/details/75212800) https://blog.csdn.net/kongfy4307/article/details/75212800

# 0x00 背景
研一浑浑噩噩,到了研二才后知后觉,需要努力,需要发论文.
导师研究光场及深度学习中的立体匹配,建议我先拿2017年的`Unsupervised Monocular Depth Estimation with Left-Right Consistency`这篇论文作为入门,单目深度估计.
先撸代码,再撸论文..
> 中文注释后的代码已经上传Github, 传送门: [https://github.com/Lebhoryi/learn_monodepth](https://github.com/Lebhoryi/learn_monodepth)




# 0x01 主函数
在main()函数里面是这样用的。

* 给定参数

* 训练或者测试

```python
def main(_):
    params = monodepth_parameters(
        encoder=args.encoder,
        height=args.input_height,
        width=args.input_width,
        batch_size=args.batch_size,
        num_threads=args.num_threads,
        num_epochs=args.num_epochs,
        do_stereo=args.do_stereo,
        wrap_mode=args.wrap_mode,
        use_deconv=args.use_deconv,
        alpha_image_loss=args.alpha_image_loss,
        disp_gradient_loss_weight=args.disp_gradient_loss_weight,
        lr_loss_weight=args.lr_loss_weight,
        full_summary=args.full_summary)
    if args.mode == 'train':
        train(params)
    elif args.mode == 'test':
        test(params)
```
```python
monodepth_parameters = namedtuple('parameters', 
                        'encoder, '
                        'height, width, '
                        'batch_size, '
                        'num_threads, '
                        'num_epochs, '
                        'do_stereo, '
                        'wrap_mode, '
                        'use_deconv, '
                        'alpha_image_loss, '
                        'disp_gradient_loss_weight, '
                        'lr_loss_weight, '
                        'full_summary')
```
monodepth_parameters: 调用`monodepth_model.py`中的`monodepth_parameters`参数，创建一个`monodepth_parameters`对象，并赋值
namedtuple: `namedtuple(typename, field_names)`, 定义一个`namedtuple`类型的`monodepth_parameters`，类似元组的对象，并包含`encoder, height, width, batch_size, num_threads, num_epochs, do_stereo, wrap_mode, use_deconv, alpha_image_loss, disp_gradient_loss_weight, lr_loss_weight, full_summary`属性。

# 0x02 加载数据
`monodepth_main.py`
```python
 # 加载数据,依次获得一个batch的数据,每次四个线程
data_loader = MonodepthDataloader(args.data_path, args.filenames_file, params, args.dataset, args.mode)
```
调用`monodepth_dataloader.py`中的`MonodepthDataloader`类函数。

直接从文件中读取数据,
1.使用tf.train.string_input_producer函数把我们需要的全部文件打包为一个tf内部的queue类型，之后tf开文件就从这个queue中取目录了，要注意一点的是这个函数的shuffle参数默认是True，也就是你传给他文件顺序是1234，但是到时候读就不一定。
2.搞一个reader，不同reader对应不同的文件结构，比如度bin文件tf.FixedLengthRecordReader就比较好，因为每次读等长的一段数据。如果要读什么别的结构也有相应的reader。
3.用reader的read方法，这个方法需要一个IO类型的参数，就是我们上边string_input_producer输出的那个queue了，reader从这个queue中取一个文件目录，然后打开它经行一次读取，reader的返回是一个tensor（这一点很重要，我们现在写的这些读取代码并不是真的在读数据，还是在画graph，和定义神经网络是一样的，这时候的操作在run之前都不会执行，这个返回的tensor也没有值，他仅仅代表graph中的一个结点）。
> key, value = reader.read(files)

4.对这个tensor做些数据与处理，比如CIFAR1-10中label和image数据是糅在一起的，这里用slice把他们切开，切成两个tensor（注意这个两个tensor是对应的，一个image对一个label，对叉了后便训练就完了），然后对image的tensor做data augmentation。 

```python
input_queue = tf.train.string_input_producer([filenames_file], shuffle=False)
line_reader = tf.TextLineReader()
_, line = line_reader.read(input_queue)

split_line = tf.string_split([line]).values
```
***
TensorFlow提供两种类型的拼接：

* `tf.concat(values, axis, name='concat')`：按照指定的**已经存在**的轴进行拼接

* `tf.stack(values, axis=0, name='stack')`：按照指定的**新建**的轴进行拼接

* `tf.slice(input_, begin, size, name=None)`：按照指定的下标范围抽取连续区域的子集

* `tf.gather(params, indices, validate_indices=None, name=None)`：按照指定的下标集合从axis=0中抽取子集，适合抽取不连续区域的子集

* `tf.split(value, num_or_size_splits, axis=0, num=None, name="split")`: 分割value, 分成value/num_or_size_splits份

* `tf.string_split(source, delimiter=' ')`: 拆分`source`是一维数组，用于将一组字符串按照`delimiter`拆分为多个元素，返回值为一个`SparseTensor`

>举例：
假如有两个字符串，`source[0]`是“hello world”，`source[1]`是“a b c”，那么输出结果如下：
> * `st.indices`： [0, 0; 0, 1; 1, 0; 1, 1; 1, 2]
> * `st.values`： ['hello', 'world', 'a', 'b', 'c']
> * `st.dense_shape`：[2, 3]



* `tf.string_join(inputs, separator=None, name=None)`：拼接

***

```python
def read_image(self, image_path):
   # tf.decode_image does not return the image size, this is an ugly workaround to handle both jpeg and png
   # Q: [0] 不知道什么意思
   path_length = string_length_tf(image_path)[0]
```

***

新的读取数据的方式, ` tf.train.shuffle_batch()`

```python
capacity = min_after_dequeue + (num_threads + a small safety margin) * batch_size
# capacity: An integer.The maximum number of elements in the queue.
# 容量: 一个整数, 队列中的最大的元素数
min_after_dequeue = 2048
capacity = min_after_dequeue + 4 * params.batch_size
# 读取一个文件并且加载一个张量中的batch_size行
# 从[left_image, right_image]利用 params.num_threads 个线程读取 params.batch_size 行
# min_after_dequeue:当一次出列操作完成后, 队列中元素的最小数量, 往往用于定义元素的混合级别
self.left_image_batch, self.right_image_batch = tf.train.shuffle_batch([left_image, right_image],
                        params.batch_size, capacity, min_after_dequeue, params.num_threads)

```

# 0x03 网络模型

***

![FlowNet](https://img-blog.csdn.net/20170716220318882?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva29uZ2Z5NDMwNw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

大概是像这样的一个东西, 这是`FlowNet`, 然后在基础上变种为`DispNet`, 适合`grand truth data`, 作者再次对`DispNet`基础上修改.有兴趣的同学可以去看看

***



在`monodepth_main.py`中:

```python
model = MonodepthModel(params, args.mode, left_splits[i], right_splits[i], reuse_variables, i)
```

调用`monodepth_model.py`中的`MonodepthModel`类:



```python
# 类的初始化函数
def __init__(self, params, mode, left, right, reuse_variables=None, model_index=0):
    self.params = params
    self.mode = mode # mode：train或者test
    self.left = left # left,right：是left_image_batch,right_image_batch。左右图以batch形式传进来
    self.right = right
    self.model_collection = ['model_' + str(model_index)]
    self.reuse_variables = reuse_variables
    self.build_model()
    self.build_outputs()
    if self.mode == 'test':
        return
    self.build_losses() # build_losses()：创建损失函数
    self.build_summaries() # build_summaries()：可视化工具

```

* left,right：是left_image_batch,right_image_batch。左右图以batch形式传进来。
* mode：train或者test。
* params：传进来很多参数设置。
* build_model()：创建模型
* build_outputs()：创建输出
* build_losses()：创建损失函数
* build_summaries()：可视化工具

***

### 创建模型`build_model(self)`:

作者主要调用了`slim`里面的基础CNN操作;

1. 生成左图金字塔：尺度为4
2. 如果训练则生成右图金字塔，如果做`stereo`，则把左右图在`channel`维上叠在一起作为模型输入。否则把左图作为模型输入
3. 根据`params`里面的设定，选择`vgg`或者`resnet50`作为编码器。

```python

# 生成左图金字塔
# Q: 什么是图片金字塔?
# A:图片金字塔就是原图+1/x的原图, 返回新的列表.
# Q: 为什么要有图片金字塔?
self.left_pyramid = self.scale_pyramid(self.left, 4)
```


***

### 编码与解码

![编码与解码](https://img-blog.csdnimg.cn/20190127095744149.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzMjI1NDM3,size_16,color_FFFFFF,t_70)

作者的网络架构，其中k是卷积核大小，s是步幅，chns每层的输入和输出通道的数量，输入和输出是每层相对于输入图像的缩减因子，并且输入对应于每个层的输入，其中+是串联和* 卷积是层的a2×上采样。



```python

if self.params.encoder == 'vgg':
    self.build_vgg()
elif self.params.encoder == 'resnet50':
    self.build_resnet50()
else:
    return None
```
1. 以VGG为例，可以看到conv1-conv7都是标准的vgg。
2. skip指的是把conv1-conv7引出来。
3. 在decoder中，upconv指采用反卷积或者上采样的方法逐步恢复原来的尺度。而skip引出的结果用来与decoder里面的feature maps在第三维也就是channel维叠起来后做upconv。这样逐步upconv
4. 最后利用iconv得到视差图。

***
### 上卷积
先做最近邻上采样, 然后做卷积，步长为1
```python
def upconv(self, x, num_out_layers, kernel_size, scale):
    upsample = self.upsample_nn(x, scale)
    conv = self.conv(upsample, num_out_layers, kernel_size, 1)
    return conv
```
### 上采样
最近邻上采样调用的是`tensorflow.image`里面的`resize_nearest_neightbor`函数。之后图片放大`ratio`倍
```python
def upsample_nn(self, x, ratio):
    s = tf.shape(x)
    h = s[1]
    w = s[2]
    return tf.image.resize_nearest_neighbor(x, [h * ratio, w * ratio])
```
### 生成视差图
作者在upconv之后用了一个CNN加sigmoid函数，乘以0.3之后作为视差图
```python
def get_disp(self, x):
    """生成视差图"""
    disp = 0.3 * self.conv(x, 2, 3, 1, tf.nn.sigmoid)
    return disp
```
***
### 生成输出
![反向采样过程](https://pic3.zhimg.com/80/v2-df9a178dcd06f3bc03ee732c6a3b6dee_hd.jpg)
```python
def build_outputs(self):
    '''一次 loop 的输出'''
    # STORE DISPARITIES
    # 生成 dr 和 dl
    with tf.variable_scope('disparities'):
        # 将四个尺度的视差图排成队列
        self.disp_est = [self.disp1, self.disp2, self.disp3, self.disp4]
        # 从视差图队列中取出左(右)视差图(最后输出的视差图的0通道是左图, 1 通道是右图),
        # 用tf.expand_dims()加上通道轴,变成[batch,height,width,1]形状的tensor
        self.disp_left_est = [tf.expand_dims(d[:,:,:,0], 3) for d in self.disp_est]
        self.disp_right_est = [tf.expand_dims(d[:,:,:,1], 3) for d in self.disp_est]

    # 如果是测试模式,之后的代码部分不运行
    if self.mode == 'test':
        return

    # 原图估计, 调用生成左(右)图估计函数
    with tf.variable_scope('images'):
        # 通过上面生成的 dr 和 dl 生成图片 I`r 和 I`l (backward sampling)
        self.left_est = [self.generate_image_left(self.right_pyramid[i], self.disp_left_est[i]) for i in range(4)]
        self.right_est = [self.generate_image_right(self.left_pyramid[i], self.disp_right_est[i]) for i in range(4)]

    # LR CONSISTENCY 左右一致性
    # 用右视差图中的视差通过视差索引找到左视差图上的点, 然后再通过做视差图点上的视差索引生成新的右视差图.
    # 就可以用右视差图和新的右视差图产生衡量一致性的项.
    with tf.variable_scope('left-right'):
        self.right_to_left_disp = [self.generate_image_left(self.disp_right_est[i], self.disp_left_est[i]) for i in range(4)]
        self.left_to_right_disp = [self.generate_image_right(self.disp_left_est[i], self.disp_right_est[i]) for i in range(4)]

    # DISPARITY SMOOTHNESS
    with tf.variable_scope('smoothness'):
        self.disp_left_smoothness = self.get_disparity_smoothness(self.disp_left_est, self.left_pyramid)
        self.disp_right_smoothness = self.get_disparity_smoothness(self.disp_right_est, self.right_pyramid)
```
主要包括四个部分:
1. 视差图:包括用来生成左图和生成右图的视差图, 生成d^r, d^l
2. 原图估计:通过左(右)原图和右(左)视差图生成右(左)图的估计, 生成图片 I`r 和 I`l
3. 一致性:通过右(左)视差图和左(右)视差图生成新的右(左)视差图:计算用来计算左右图一致性, 
4. 平滑性:通过左(右)原图和左(右)图估计计算平滑项

***
### 生成损失
![损失](https://img-blog.csdnimg.cn/20190127094753510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzMjI1NDM3,size_16,color_FFFFFF,t_70)
损失函数包括以下几个部分：
1. 原图和重建的图之间的差异，用L1范数表示
2. 原图和重建的图的SSIM，并且左右图加权求和
3. 视差图平滑损失
4. 左右图一致性损失
```python
self.total_loss = self.image_loss + self.params.disp_gradient_loss_weight * self.disp_gradient_loss + self.params.lr_loss_weight * self.lr_loss
```

***
**不足:**
1. 精髓部分, 作者改进的损失函数部分没有仔细考察;
2. 双线性差值不是很理解;
3. Tensorflow中的队列尚未仔细揣摩;
4. 测试部分代码未查阅


